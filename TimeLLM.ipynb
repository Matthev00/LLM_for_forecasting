{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "\n",
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "\n",
    "\n",
    "def time_features(dates, freq=\"h\"):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class ETThour_Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_path: Path = Path(\"data/dataset/ETT-small\"),\n",
    "        flag: str = \"train\",\n",
    "        size: Tuple[int, int, int] = None,\n",
    "        features: str = \"S\",\n",
    "        data_name: str = \"ETTh1.csv\",\n",
    "        target: str = \"OT\",\n",
    "        scale: bool = True,\n",
    "        timeenc: int = 0,\n",
    "        freq: str = \"h\",\n",
    "        percent: int = 100,\n",
    "    ):\n",
    "        if size is None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "\n",
    "        assert flag in [\"train\", \"val\", \"test\"]\n",
    "        type_map = {\"train\": 0, \"val\": 1, \"test\": 2}\n",
    "        self.type = type_map[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.percent = percent\n",
    "\n",
    "        self.data_path = root_path / data_name\n",
    "        self.__read_data__()\n",
    "\n",
    "        self.enc_in = self.data_x.shape[-1]\n",
    "        self.tot_len = len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(self.data_path)\n",
    "\n",
    "        start_borders = [\n",
    "            0,  # train_start\n",
    "            12 * 30 * 24 - self.seq_len,  # val_start\n",
    "            12 * 30 * 24 + 4 * 30 * 24 - self.seq_len,  # test_start\n",
    "        ]\n",
    "        end_borders = [\n",
    "            12 * 30 * 24,  # train_end\n",
    "            12 * 30 * 24 + 4 * 30 * 24,  # val_end\n",
    "            12 * 30 * 24 + 8 * 30 * 24,  # test_end\n",
    "        ]\n",
    "\n",
    "        start_border = start_borders[self.type]\n",
    "        end_border = end_borders[self.type]\n",
    "\n",
    "        if self.type == 0:\n",
    "            end_border = (end_border - self.seq_len) * self.percent // 100 + self.seq_len\n",
    "\n",
    "        if self.features == \"M\" or self.features == \"MS\":\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == \"S\":\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[start_borders[0] : end_borders[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[[\"date\"]][start_border:end_border]\n",
    "        df_stamp[\"date\"] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp[\"month\"] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp[\"day\"] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp[\"weekday\"] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp[\"hour\"] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            df_stamp = df_stamp.drop([\"date\"], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(\n",
    "                pd.to_datetime(df_stamp[\"date\"].values), freq=self.freq\n",
    "            )\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[start_border:end_border]\n",
    "        self.data_y = data[start_border:end_border]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data_x) - self.seq_len - self.pred_len + 1) * self.enc_in\n",
    "\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        feat_id = index // self.tot_len\n",
    "        s_begin = index % self.tot_len\n",
    "\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "        seq_x = self.data_x[s_begin:s_end, feat_id : feat_id + 1]\n",
    "        seq_y = self.data_y[r_begin:r_end, feat_id : feat_id + 1]\n",
    "\n",
    "        return seq_x, seq_y\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple\n",
    "\n",
    "data_dict = {\"ETT\": ETThour_Dataset}\n",
    "\n",
    "\n",
    "def data_provider(args, flag: str = \"train\") -> Tuple[ETThour_Dataset, DataLoader]:\n",
    "    Data = data_dict[args.data]\n",
    "    timeenc = 0 if args.embed != \"timeF\" else 1\n",
    "    percent = args.percent\n",
    "\n",
    "    if flag == \"test\":\n",
    "        shuffle_flag = False\n",
    "        drop_last = False\n",
    "    else:\n",
    "        shuffle_flag = True\n",
    "        drop_last = True\n",
    "    batch_size = args.batch_size\n",
    "    freq = args.freq\n",
    "\n",
    "    data_set = Data(\n",
    "        # root_path=args.root_path,\n",
    "        flag=flag,\n",
    "        size=(args.seq_len, args.label_len, args.pred_len),\n",
    "        features=args.features,\n",
    "        target=args.target,\n",
    "        timeenc=timeenc,\n",
    "        freq=freq,\n",
    "        percent=percent,\n",
    "    )\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_flag,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "    return data_set, data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "\n",
    "class ReplicationPad1d(nn.Module):\n",
    "    def __init__(self, padding) -> None:\n",
    "        super(ReplicationPad1d, self).__init__()\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        replicate_padding = x[:, :, -1].unsqueeze(-1).repeat(1, 1, self.padding[-1])\n",
    "        output = torch.cat([x, replicate_padding], dim=-1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TokenEmbedder(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedder, self).__init__()\n",
    "        self.tokenconv = nn.Conv1d(\n",
    "            in_channels=c_in,\n",
    "            out_channels=d_model,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode=\"circular\",\n",
    "            bias=False,\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode=\"fan_in\", nonlinearity=\"leaky_relu\"\n",
    "                )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.to(torch.float32)\n",
    "        return self.tokenconv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "\n",
    "\n",
    "class PatchEmbedder(nn.Module):\n",
    "    def __init__(self, d_model, patch_len, stride, dropout):\n",
    "        super(PatchEmbedder, self).__init__()\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.padding_patch_layer = ReplicationPad1d((0, stride))\n",
    "        self.value_embedding = TokenEmbedder(patch_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_vars = x.shape[1]\n",
    "        x = self.padding_patch_layer(x)\n",
    "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
    "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n",
    "        x = self.value_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        return x, n_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, nf, target_window, head_dropout):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        return self.dropout(self.linear(self.flatten(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps=1e-5,\n",
    "        affine=False,\n",
    "        subtract_last=False,\n",
    "        non_norm=False,\n",
    "    ):\n",
    "        super(NormalizeLayer, self).__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        self.non_norm = non_norm\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode: str) -> Tensor:\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:, -1, :].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.non_norm:\n",
    "            return x\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.non_norm:\n",
    "            return x\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps * self.eps)\n",
    "        x = x * self.stdev\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last\n",
    "        else:\n",
    "            x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "\n",
    "class ReprogrammingLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1\n",
    "    ):\n",
    "        super(ReprogrammingLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.value = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.out = nn.Linear(d_keys * n_heads, d_llm)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, target_embedding, source_embedding, value_embedding) -> Tensor:\n",
    "        B, L, _ = target_embedding.shape\n",
    "        S, _ = source_embedding.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        target_embedding = self.query(target_embedding).view(B, L, H, -1)\n",
    "        source_embedding = self.key(source_embedding).view(S, H, -1)\n",
    "        value_embedding = self.value(value_embedding).view(S, H, -1)\n",
    "\n",
    "        out = self.reprogramming(target_embedding, source_embedding, value_embedding)\n",
    "\n",
    "        out = out.reshape(B, L, -1)\n",
    "\n",
    "        return self.out(out)\n",
    "\n",
    "    def reprogramming(\n",
    "        self, target_embedding, source_embedding, value_embedding\n",
    "    ) -> Tensor:\n",
    "        B, L, H, E = target_embedding.shape\n",
    "\n",
    "        scale = 1.0 / sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,she->bhls\", target_embedding, source_embedding)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        reprogramming_embedding = torch.einsum(\"bhls,she->blhe\", A, value_embedding)\n",
    "\n",
    "        return reprogramming_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2Model, GPT2Tokenizer\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class TimeLLM(nn.Module):\n",
    "    def __init__(self, configs=None):\n",
    "        super(TimeLLM, self).__init__()\n",
    "\n",
    "        self.configs = configs\n",
    "\n",
    "        self._set_llm_model()\n",
    "        self._set_tokenizer()\n",
    "        self._set_pad_token()\n",
    "        self._freeze_llm()\n",
    "        self.description = configs.content\n",
    "\n",
    "        # layers\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "        self.patch_embedder = PatchEmbedder(\n",
    "            configs.d_model, configs.patch_len, configs.stride, configs.dropout\n",
    "        )\n",
    "\n",
    "        self.word_embeddings = self.llm_model.get_input_embeddings().weight\n",
    "        vocab_size = self.word_embeddings.shape[0]\n",
    "        num_tokens = 1000\n",
    "        self.mapping_layer = nn.Linear(vocab_size, num_tokens)\n",
    "\n",
    "        self.reprogramming_layer = ReprogrammingLayer(\n",
    "            configs.d_model, configs.n_heads, configs.d_ff, configs.llm_dim\n",
    "        )\n",
    "\n",
    "        self.patch_nums = int(\n",
    "            (configs.seq_len - configs.patch_len) / configs.stride + 2\n",
    "        )\n",
    "        self.head_nf = configs.d_ff * self.patch_nums\n",
    "\n",
    "        self.output_projection = FlattenHead(\n",
    "            self.head_nf, configs.pred_len, head_dropout=configs.dropout\n",
    "        )\n",
    "\n",
    "        self.normalize_layer = NormalizeLayer(configs.enc_in, affine=False)\n",
    "\n",
    "    def forward(self, x_enc) -> Tensor:\n",
    "        dec_out = self.forecast(x_enc)\n",
    "        return dec_out[:, -self.configs.pred_len :, :]\n",
    "\n",
    "    def forecast(self, x_enc) -> Tensor:\n",
    "        x_enc = self.normalize_layer(x_enc, mode=\"norm\")\n",
    "\n",
    "        B, T, N = x_enc.size()\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n",
    "        prompt = self._generate_prompt(x_enc)\n",
    "        x_enc = x_enc.reshape(B, N, T).permute(0, 2, 1).contiguous()\n",
    "\n",
    "        prompt = self.tokenizer(\n",
    "            prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048\n",
    "        ).input_ids\n",
    "        prompt_embeddings = self.llm_model.get_input_embeddings()(\n",
    "            prompt.to(x_enc.device)\n",
    "        )  # (batch, prompt_token, dim)\n",
    "\n",
    "        source_embeddings = self.mapping_layer(\n",
    "            self.word_embeddings.permute(1, 0)\n",
    "        ).permute(1, 0)\n",
    "\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous()\n",
    "        enc_out, n_vars = self.patch_embedder(x_enc.to(torch.bfloat16))\n",
    "        enc_out = self.reprogramming_layer(\n",
    "            enc_out, source_embeddings, source_embeddings\n",
    "        )\n",
    "        llm_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\n",
    "        dec_out = self.llm_model(inputs_embeds=llm_enc_out).last_hidden_state\n",
    "        dec_out = dec_out[:, :, : self.configs.d_ff]\n",
    "\n",
    "        dec_out = torch.reshape(\n",
    "            dec_out, (-1, n_vars, dec_out.shape[-2], dec_out.shape[-1])\n",
    "        )\n",
    "        dec_out = dec_out.permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "        dec_out = self.output_projection(dec_out[:, :, :, -self.patch_nums :])\n",
    "        dec_out = dec_out.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        dec_out = self.normalize_layer(dec_out, \"denorm\")\n",
    "\n",
    "        return dec_out\n",
    "\n",
    "    def _calculate_lags(self, x_enc: Tensor, top_k: int = 5) -> Tensor:\n",
    "        q_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "        mean_value = torch.mean(corr, dim=1)\n",
    "        _, lags = torch.topk(mean_value, top_k, dim=-1)\n",
    "        return lags\n",
    "\n",
    "    def _generate_prompt(self, x_enc) -> List[str]:\n",
    "        min_values = torch.min(x_enc, dim=1)[0]\n",
    "        max_value = torch.max(x_enc, dim=1)[0]\n",
    "        medians = torch.max(x_enc, dim=1).values\n",
    "        lags = self._calculate_lags(x_enc)\n",
    "        trends = x_enc.diff(dim=1).sum(dim=1)\n",
    "\n",
    "        prompt = []\n",
    "        for batch in range(x_enc.shape[0]):\n",
    "            min_values_str = str(min_values[batch].tolist()[0])\n",
    "            max_values_str = str(max_value[batch].tolist()[0])\n",
    "            medians_str = str(medians[batch].tolist()[0])\n",
    "            lags_str = str(lags[batch].tolist())\n",
    "            prompt_ = (\n",
    "                f\"<|start_prompt|>Dataset description: {self.description}\"\n",
    "                f\"Task description: forecast the next {str(self.configs.pred_len)} steps given the previous {str(self.configs.seq_len)} steps information; \"\n",
    "                \"Input statistics: \"\n",
    "                f\"min value {min_values_str}, \"\n",
    "                f\"max value {max_values_str}, \"\n",
    "                f\"median value {medians_str}, \"\n",
    "                f\"the trend of input is {'upward' if trends[batch] > 0 else 'downward'}, \"\n",
    "                f\"top 5 lags are : {lags_str}<|<end_prompt>|>\"\n",
    "            )\n",
    "            prompt.append(prompt_)\n",
    "        return prompt\n",
    "\n",
    "    def _set_llm_model(self):\n",
    "        config = GPT2Config.from_pretrained(\"openai-community/gpt2\")\n",
    "        config.num_hidden_layers = self.configs.llm_layers\n",
    "        config.output_attentions = True\n",
    "        config.output_hidden_states = True\n",
    "        try:\n",
    "            self.llm_model = GPT2Model.from_pretrained(\n",
    "                \"openai-community/gpt2\",\n",
    "                config=config,\n",
    "                trust_remote_code=True,\n",
    "                local_files_only=True,\n",
    "            )\n",
    "        except EnvironmentError:\n",
    "            print(\"Local model files not found. Attempting to download...\")\n",
    "            self.llm_model = GPT2Model.from_pretrained(\n",
    "                \"openai-community/gpt2\",\n",
    "                config=config,\n",
    "                trust_remote_code=True,\n",
    "                local_files_only=False,\n",
    "            )\n",
    "\n",
    "    def _set_tokenizer(self):\n",
    "        try:\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                \"openai-community/gpt2\",\n",
    "                trust_remote_code=True,\n",
    "                local_files_only=True,\n",
    "            )\n",
    "        except EnvironmentError:\n",
    "            print(\"Local tokenizer files not found. Attempting to download...\")\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                \"openai-community/gpt2\",\n",
    "                trust_remote_code=True,\n",
    "                local_files_only=False,\n",
    "            )\n",
    "\n",
    "    def _set_pad_token(self):\n",
    "        if self.tokenizer.eos_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        else:\n",
    "            pad_token = \"[PAD]\"\n",
    "            self.tokenizer.add_special_tokens({\"pad_token\": pad_token})\n",
    "            self.tokenizer.pad_token = pad_token\n",
    "\n",
    "    def _freeze_llm(self):\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mateu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter  # noqa 5501\n",
    "\n",
    "from utils import adjust_lr\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, List\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "def train(\n",
    "    args,\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    valid_loader: torch.utils.data.DataLoader,\n",
    "    optim: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler.OneCycleLR,\n",
    "    criterion: torch.nn.Module,\n",
    "    mae_metric: torch.nn.Module,\n",
    "    epochs: int = 10,\n",
    "    device: torch.device = \"cuda:0\",\n",
    "    writer: SummaryWriter = None,\n",
    ") -> Dict[str, List[float]]:\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"valid_loss\": [],\n",
    "        \"mae_loss\": [],\n",
    "    }\n",
    "    for epoch in range(epochs):\n",
    "        start = timer()\n",
    "        train_loss = train_step(\n",
    "            args=args,\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            optim=optim,\n",
    "            scheduler=scheduler,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "        )\n",
    "        adjust_lr(optim, scheduler, epoch, args)\n",
    "\n",
    "        valid_loss, mae_loss = valid_step(\n",
    "            args=args,\n",
    "            model=model,\n",
    "            valid_loader=valid_loader,\n",
    "            criterion=criterion,\n",
    "            mae_metric=mae_metric,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"valid_loss\"].append(valid_loss)\n",
    "        results[\"mae_loss\"].append(mae_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\\nTime: {timer()-start}\")\n",
    "        print(f\"Train Loss: {train_loss}\\nValid Loss: {valid_loss}\")\n",
    "\n",
    "        if writer:\n",
    "            writer.add_scalars(\n",
    "                main_tag=\"Loss\",\n",
    "                tag_scalar_dict={\"train_loss\": train_loss, \"test_loss\": valid_loss},\n",
    "                global_step=epoch,\n",
    "            )\n",
    "\n",
    "            writer.add_scalars(\n",
    "                main_tag=\"MAE Loss\",\n",
    "                tag_scalar_dict={\"mae_loss\": mae_loss},\n",
    "                global_step=epoch,\n",
    "            )\n",
    "    if writer:\n",
    "        writer.close()\n",
    "    return results\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    args,\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optim: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler.OneCycleLR,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device = \"cuda:0\",\n",
    ") -> float:\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    print(\"Training...\")\n",
    "    for batch_idx, (batch_x, batch_y) in tqdm(enumerate(dataloader)):\n",
    "        print(f\"Batch {batch_idx + 1}/{len(dataloader)} loaded.\")\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y[-1, :, :].float().to(device)\n",
    "\n",
    "        outputs = model(batch_x)[0]\n",
    "\n",
    "        f_dim = -1 if args.features == \"MS\" else 0\n",
    "        outputs = outputs[-args.pred_len :, f_dim:]\n",
    "        batch_y = batch_y[-args.pred_len :, f_dim:]\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def valid_step(\n",
    "    args,\n",
    "    model: torch.nn.Module,\n",
    "    valid_loader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    mae_metric: torch.nn.Module,\n",
    "    device: torch.device = \"cuda:0\",\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    valid_loss, mae_loss = 0.0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"Testing...\")\n",
    "        for i, (batch_x, batch_y) in tqdm(enumerate(valid_loader)):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y[-1, :, :].float().to(device)\n",
    "\n",
    "            outputs = model(batch_x)[0]\n",
    "\n",
    "            f_dim = -1 if args.features == \"MS\" else 0\n",
    "            outputs = outputs[-args.pred_len :, f_dim:]\n",
    "            batch_y = batch_y[-args.pred_len :, f_dim:]\n",
    "\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            valid_loss += loss.item()\n",
    "            mae_loss += mae_metric(outputs, batch_y).item()\n",
    "\n",
    "    return valid_loss / len(valid_loader), mae_loss / len(valid_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def set_seeds(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def parse_argument():\n",
    "    parser = argparse.ArgumentParser(description=\"TimeLLM\")\n",
    "    # data\n",
    "    parser.add_argument(\"--data\", type=str, default=\"ETT\", help=\"dataset type\")\n",
    "    parser.add_argument(\n",
    "        \"--embed\",\n",
    "        type=str,\n",
    "        default=\"timeF\",\n",
    "        help=\"time features encoding, options:[timeF, fixed, learned]\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--freq\", type=str, default=\"h\", help=\"freq for time features encoding\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--features\",\n",
    "        type=str,\n",
    "        default=\"M\",\n",
    "        help=\"forecasting task, options:[M, S, MS]; \"\n",
    "        \"M:multivariate predict multivariate, S: univariate predict univariate, \"\n",
    "        \"MS:multivariate predict univariate\",\n",
    "    )\n",
    "    parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "\n",
    "    # forecasting task\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=96, help=\"input sequence length\")\n",
    "    parser.add_argument(\"--label_len\", type=int, default=48, help=\"start token length\")\n",
    "    parser.add_argument(\n",
    "        \"--pred_len\", type=int, default=96, help=\"prediction sequence length\"\n",
    "    )\n",
    "\n",
    "    # model define\n",
    "    parser.add_argument(\"--enc_in\", type=int, default=7, help=\"encoder input size\")\n",
    "    parser.add_argument(\"--d_model\", type=int, default=16, help=\"dimension of model\")\n",
    "    parser.add_argument(\"--n_heads\", type=int, default=8)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--patch_len\", type=int, default=16)\n",
    "    parser.add_argument(\"--stride\", type=int, default=8)\n",
    "    parser.add_argument(\"--llm_layers\", type=int, default=6)\n",
    "    parser.add_argument(\"--d_ff\", type=int, default=32, help=\"dimension of fcn\")\n",
    "    parser.add_argument(\n",
    "        \"--llm_dim\", type=int, default=\"768\", help=\"LLM model dimension\"\n",
    "    )\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument(\"--train_epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\n",
    "        \"--patience\", type=int, default=10, help=\"early stopping patience\"\n",
    "    )\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.0001)\n",
    "    parser.add_argument(\"--pct_start\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--percent\", type=int, default=100)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def adjust_lr(\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler,\n",
    "    epoch: int,\n",
    "    args,\n",
    "):\n",
    "    new_lr = args.learning_rate * (0.5 ** ((epoch - 1) // 1))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = new_lr\n",
    "\n",
    "\n",
    "def load_content(args):\n",
    "    file = args.data\n",
    "    with open(\"./data/prompt_bank/{0}.txt\".format(file), \"r\") as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "\n",
    "def save_model(model: torch.nn.Module, model_name:str):\n",
    "    dir_path = Path(\"./model/pretrained\")\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model.state_dict(), dir_path / model_name)\n",
    "\n",
    "\n",
    "def test_data_loading(train_loader):\n",
    "    print(\"Testing data loading...\")\n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        print(f\"Batch {batch_idx + 1}/{len(train_loader)} loaded.\")\n",
    "        if batch_idx >= 5:  # Testuj tylko kilka batchy\n",
    "            break\n",
    "    print(\"Data loading test completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, percent):\n",
    "        # Dataset parameters\n",
    "        self.data = \"ETT\"\n",
    "        self.embed = \"timeF\"\n",
    "        self.freq = \"h\"\n",
    "        self.features = \"M\"\n",
    "        self.target = \"OT\"\n",
    "\n",
    "        # Forecasting task parameters\n",
    "        self.seq_len = 96\n",
    "        self.label_len = 48\n",
    "        self.pred_len = 96\n",
    "        self.enc_in = 7\n",
    "        self.d_model = 16\n",
    "        self.n_heads = 8\n",
    "        self.dropout = 0.1\n",
    "        self.patch_len = 16\n",
    "        self.stride = 8\n",
    "        self.llm_layers = 6\n",
    "        self.d_ff = 32\n",
    "        self.llm_dim = 768\n",
    "\n",
    "        # Optimization parameters\n",
    "        self.train_epochs = 10\n",
    "        self.batch_size = 32\n",
    "        self.patience = 10\n",
    "        self.learning_rate = 0.0001\n",
    "        self.pct_start = 0.2\n",
    "        self.percent = percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:05, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 11152) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 57\u001b[0m\n\u001b[0;32m     53\u001b[0m     save_model(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_llm_test.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 40\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     38\u001b[0m mae_metric \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss()\n\u001b[1;32m---> 40\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmae_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmae_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvali_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m save_model(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_llm_test.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, model, train_loader, valid_loader, optim, scheduler, criterion, mae_metric, epochs, device, writer)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     30\u001b[0m     start \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m---> 31\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     adjust_lr(optim, scheduler, epoch, args)\n\u001b[0;32m     42\u001b[0m     valid_loss, mae_loss \u001b[38;5;241m=\u001b[39m valid_step(\n\u001b[0;32m     43\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m     44\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     49\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[15], line 87\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(args, model, dataloader, optim, scheduler, criterion, device)\u001b[0m\n\u001b[0;32m     85\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (batch_x, batch_y) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader)):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m     batch_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 11152) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def main():\n",
    "    set_seeds()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    args = Args(5)\n",
    "    args.content = load_content(args)\n",
    "    args.num_workers = 1\n",
    "    model = TimeLLM(args).float().to(device)\n",
    "\n",
    "    train_data, train_loader = data_provider(args, \"train\")\n",
    "    vali_data, vali_loader = data_provider(args, \"val\")\n",
    "    test_data, test_loader = data_provider(args, \"test\")\n",
    "    train_steps = len(train_loader)\n",
    "\n",
    "    # print(next(iter(train_loader))[0])\n",
    "    # test_data_loading(train_loader)\n",
    "\n",
    "    trained_parameters = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad is True:\n",
    "            trained_parameters.append(p)\n",
    "\n",
    "    optim = torch.optim.Adam(trained_parameters, lr=args.learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer=optim,\n",
    "        steps_per_epoch=train_steps,\n",
    "        pct_start=args.pct_start,\n",
    "        epochs=args.train_epochs,\n",
    "        max_lr=args.learning_rate,\n",
    "    )\n",
    "    criterion = nn.MSELoss()\n",
    "    mae_metric = nn.L1Loss()\n",
    "\n",
    "    train(\n",
    "        args=args,\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        optim=optim,\n",
    "        scheduler=scheduler,\n",
    "        criterion=criterion,\n",
    "        mae_metric=mae_metric,\n",
    "        device=device,\n",
    "        valid_loader=vali_loader,\n",
    "        epochs=10,\n",
    "    )\n",
    "\n",
    "    save_model(model, \"time_llm_test.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
