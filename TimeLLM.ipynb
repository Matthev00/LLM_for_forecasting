{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matthev00/LLM_for_forecasting/blob/master/TimeLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3UTxcQT7UrB"
      },
      "source": [
        "# DATA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuQ9KMCK5mh7",
        "outputId": "50026ca3-b978-4514-a73d-d5dce2428dfa"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "UMT9k79A7UrD"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.tseries import offsets\n",
        "from pandas.tseries.frequencies import to_offset\n",
        "\n",
        "\n",
        "class TimeFeature:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \"()\"\n",
        "\n",
        "\n",
        "class SecondOfMinute(TimeFeature):\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.second / 59.0 - 0.5\n",
        "\n",
        "\n",
        "class MinuteOfHour(TimeFeature):\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.minute / 59.0 - 0.5\n",
        "\n",
        "\n",
        "class HourOfDay(TimeFeature):\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.hour / 23.0 - 0.5\n",
        "\n",
        "\n",
        "class DayOfWeek(TimeFeature):\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.dayofweek / 6.0 - 0.5\n",
        "\n",
        "\n",
        "class DayOfMonth(TimeFeature):\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.day - 1) / 30.0 - 0.5\n",
        "\n",
        "\n",
        "class DayOfYear(TimeFeature):\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
        "\n",
        "\n",
        "class MonthOfYear(TimeFeature):\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.month - 1) / 11.0 - 0.5\n",
        "\n",
        "\n",
        "class WeekOfYear(TimeFeature):\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
        "\n",
        "\n",
        "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
        "    features_by_offsets = {\n",
        "        offsets.YearEnd: [],\n",
        "        offsets.QuarterEnd: [MonthOfYear],\n",
        "        offsets.MonthEnd: [MonthOfYear],\n",
        "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
        "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.Minute: [\n",
        "            MinuteOfHour,\n",
        "            HourOfDay,\n",
        "            DayOfWeek,\n",
        "            DayOfMonth,\n",
        "            DayOfYear,\n",
        "        ],\n",
        "        offsets.Second: [\n",
        "            SecondOfMinute,\n",
        "            MinuteOfHour,\n",
        "            HourOfDay,\n",
        "            DayOfWeek,\n",
        "            DayOfMonth,\n",
        "            DayOfYear,\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    offset = to_offset(freq_str)\n",
        "\n",
        "    for offset_type, feature_classes in features_by_offsets.items():\n",
        "        if isinstance(offset, offset_type):\n",
        "            return [cls() for cls in feature_classes]\n",
        "\n",
        "    supported_freq_msg = f\"\"\"\n",
        "    Unsupported frequency {freq_str}\n",
        "    The following frequencies are supported:\n",
        "        Y   - yearly\n",
        "            alias: A\n",
        "        M   - monthly\n",
        "        W   - weekly\n",
        "        D   - daily\n",
        "        B   - business days\n",
        "        H   - hourly\n",
        "        T   - minutely\n",
        "            alias: min\n",
        "        S   - secondly\n",
        "    \"\"\"\n",
        "    raise RuntimeError(supported_freq_msg)\n",
        "\n",
        "\n",
        "def time_features(dates, freq=\"h\"):\n",
        "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "shY0r8aT7UrE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "class ETThour_Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root_path: Path = Path(\"/content/drive/My Drive/TimeLLM\"),\n",
        "        flag: str = \"train\",\n",
        "        size: Tuple[int, int, int] = None,\n",
        "        features: str = \"S\",\n",
        "        data_name: str = \"ETTh1.csv\",\n",
        "        target: str = \"OT\",\n",
        "        scale: bool = True,\n",
        "        timeenc: int = 0,\n",
        "        freq: str = \"h\",\n",
        "        percent: int = 100,\n",
        "    ):\n",
        "        if size is None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "\n",
        "        assert flag in [\"train\", \"val\", \"test\"]\n",
        "        type_map = {\"train\": 0, \"val\": 1, \"test\": 2}\n",
        "        self.type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.percent = percent\n",
        "\n",
        "        self.data_path = root_path / data_name\n",
        "        self.__read_data__()\n",
        "\n",
        "        self.enc_in = self.data_x.shape[-1]\n",
        "        self.tot_len = len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(self.data_path)\n",
        "\n",
        "        start_borders = [\n",
        "            0,  # train_start\n",
        "            12 * 30 * 24 - self.seq_len,  # val_start\n",
        "            12 * 30 * 24 + 4 * 30 * 24 - self.seq_len,  # test_start\n",
        "        ]\n",
        "        end_borders = [\n",
        "            12 * 30 * 24,  # train_end\n",
        "            12 * 30 * 24 + 4 * 30 * 24,  # val_end\n",
        "            12 * 30 * 24 + 8 * 30 * 24,  # test_end\n",
        "        ]\n",
        "\n",
        "        start_border = start_borders[self.type]\n",
        "        end_border = end_borders[self.type]\n",
        "\n",
        "        if self.type == 0:\n",
        "            end_border = (end_border - self.seq_len) * self.percent // 100 + self.seq_len\n",
        "\n",
        "        if self.features == \"M\" or self.features == \"MS\":\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == \"S\":\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            train_data = df_data[start_borders[0] : end_borders[0]]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[[\"date\"]][start_border:end_border]\n",
        "        df_stamp[\"date\"] = pd.to_datetime(df_stamp.date)\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp[\"month\"] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp[\"day\"] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp[\"weekday\"] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp[\"hour\"] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            df_stamp = df_stamp.drop([\"date\"], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(\n",
        "                pd.to_datetime(df_stamp[\"date\"].values), freq=self.freq\n",
        "            )\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[start_border:end_border]\n",
        "        self.data_y = data[start_border:end_border]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.data_x) - self.seq_len - self.pred_len + 1) * self.enc_in\n",
        "\n",
        "    def __getitem__(\n",
        "        self, index: int\n",
        "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "        feat_id = index // self.tot_len\n",
        "        s_begin = index % self.tot_len\n",
        "\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "        seq_x = self.data_x[s_begin:s_end, feat_id : feat_id + 1]\n",
        "        seq_y = self.data_y[r_begin:r_end, feat_id : feat_id + 1]\n",
        "\n",
        "        return seq_x, seq_y\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "hwbNousE7UrF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from typing import Tuple\n",
        "\n",
        "data_dict = {\"ETT\": ETThour_Dataset}\n",
        "\n",
        "\n",
        "def data_provider(args, flag: str = \"train\") -> Tuple[ETThour_Dataset, DataLoader]:\n",
        "    Data = data_dict[args.data]\n",
        "    timeenc = 0 if args.embed != \"timeF\" else 1\n",
        "    percent = args.percent\n",
        "\n",
        "    if flag == \"test\":\n",
        "        shuffle_flag = False\n",
        "        drop_last = False\n",
        "    else:\n",
        "        shuffle_flag = True\n",
        "        drop_last = True\n",
        "    batch_size = args.batch_size\n",
        "    freq = args.freq\n",
        "\n",
        "    data_set = Data(\n",
        "        # root_path=args.root_path,\n",
        "        flag=flag,\n",
        "        size=(args.seq_len, args.label_len, args.pred_len),\n",
        "        features=args.features,\n",
        "        target=args.target,\n",
        "        timeenc=timeenc,\n",
        "        freq=freq,\n",
        "        percent=percent,\n",
        "    )\n",
        "\n",
        "    data_loader = DataLoader(\n",
        "        data_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle_flag,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=drop_last,\n",
        "    )\n",
        "    return data_set, data_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npfVaHnZ7UrF"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "0iJJhbnD7UrG"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch\n",
        "\n",
        "\n",
        "class ReplicationPad1d(nn.Module):\n",
        "    def __init__(self, padding) -> None:\n",
        "        super(ReplicationPad1d, self).__init__()\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        replicate_padding = x[:, :, -1].unsqueeze(-1).repeat(1, 1, self.padding[-1])\n",
        "        output = torch.cat([x, replicate_padding], dim=-1)\n",
        "        return output\n",
        "\n",
        "\n",
        "class TokenEmbedder(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(TokenEmbedder, self).__init__()\n",
        "        self.tokenconv = nn.Conv1d(\n",
        "            in_channels=c_in,\n",
        "            out_channels=d_model,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            padding_mode=\"circular\",\n",
        "            bias=False,\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight, mode=\"fan_in\", nonlinearity=\"leaky_relu\"\n",
        "                )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = x.to(torch.float32)\n",
        "        return self.tokenconv(x.permute(0, 2, 1)).transpose(1, 2)\n",
        "\n",
        "\n",
        "class PatchEmbedder(nn.Module):\n",
        "    def __init__(self, d_model, patch_len, stride, dropout):\n",
        "        super(PatchEmbedder, self).__init__()\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.padding_patch_layer = ReplicationPad1d((0, stride))\n",
        "        self.value_embedding = TokenEmbedder(patch_len, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n_vars = x.shape[1]\n",
        "        x = self.padding_patch_layer(x)\n",
        "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
        "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n",
        "        x = self.value_embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        return x, n_vars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "L_H6gRVx7UrG"
      },
      "outputs": [],
      "source": [
        "class FlattenHead(nn.Module):\n",
        "    def __init__(self, nf, target_window, head_dropout):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten(start_dim=-2)\n",
        "        self.linear = nn.Linear(nf, target_window)\n",
        "        self.dropout = nn.Dropout(head_dropout)\n",
        "\n",
        "    def forward(self, x) -> Tensor:\n",
        "        return self.dropout(self.linear(self.flatten(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "cM2sTu-P7UrG"
      },
      "outputs": [],
      "source": [
        "class NormalizeLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_features: int,\n",
        "        eps=1e-5,\n",
        "        affine=False,\n",
        "        subtract_last=False,\n",
        "        non_norm=False,\n",
        "    ):\n",
        "        super(NormalizeLayer, self).__init__()\n",
        "\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.subtract_last = subtract_last\n",
        "        self.non_norm = non_norm\n",
        "        if self.affine:\n",
        "            self._init_params()\n",
        "\n",
        "    def forward(self, x, mode: str) -> Tensor:\n",
        "        if mode == 'norm':\n",
        "            self._get_statistics(x)\n",
        "            x = self._normalize(x)\n",
        "        elif mode == 'denorm':\n",
        "            x = self._denormalize(x)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return x\n",
        "\n",
        "    def _init_params(self):\n",
        "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
        "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
        "\n",
        "    def _get_statistics(self, x):\n",
        "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
        "        if self.subtract_last:\n",
        "            self.last = x[:, -1, :].unsqueeze(1)\n",
        "        else:\n",
        "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
        "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
        "\n",
        "    def _normalize(self, x):\n",
        "        if self.non_norm:\n",
        "            return x\n",
        "        if self.subtract_last:\n",
        "            x = x - self.last\n",
        "        else:\n",
        "            x = x - self.mean\n",
        "        x = x / self.stdev\n",
        "        if self.affine:\n",
        "            x = x * self.affine_weight\n",
        "            x = x + self.affine_bias\n",
        "        return x\n",
        "\n",
        "    def _denormalize(self, x):\n",
        "        if self.non_norm:\n",
        "            return x\n",
        "        if self.affine:\n",
        "            x = x - self.affine_bias\n",
        "            x = x / (self.affine_weight + self.eps * self.eps)\n",
        "        x = x * self.stdev\n",
        "        if self.subtract_last:\n",
        "            x = x + self.last\n",
        "        else:\n",
        "            x = x + self.mean\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "WZ4d-25S7UrH"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "\n",
        "\n",
        "class ReprogrammingLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1\n",
        "    ):\n",
        "        super(ReprogrammingLayer, self).__init__()\n",
        "\n",
        "        d_keys = d_keys or (d_model // n_heads)\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.key = nn.Linear(d_llm, d_keys * n_heads)\n",
        "        self.value = nn.Linear(d_llm, d_keys * n_heads)\n",
        "        self.out = nn.Linear(d_keys * n_heads, d_llm)\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "\n",
        "    def forward(self, target_embedding, source_embedding, value_embedding) -> Tensor:\n",
        "        B, L, _ = target_embedding.shape\n",
        "        S, _ = source_embedding.shape\n",
        "        H = self.n_heads\n",
        "\n",
        "        target_embedding = self.query(target_embedding).view(B, L, H, -1)\n",
        "        source_embedding = self.key(source_embedding).view(S, H, -1)\n",
        "        value_embedding = self.value(value_embedding).view(S, H, -1)\n",
        "\n",
        "        out = self.reprogramming(target_embedding, source_embedding, value_embedding)\n",
        "\n",
        "        out = out.reshape(B, L, -1)\n",
        "\n",
        "        return self.out(out)\n",
        "\n",
        "    def reprogramming(\n",
        "        self, target_embedding, source_embedding, value_embedding\n",
        "    ) -> Tensor:\n",
        "        B, L, H, E = target_embedding.shape\n",
        "\n",
        "        scale = 1.0 / sqrt(E)\n",
        "\n",
        "        scores = torch.einsum(\"blhe,she->bhls\", target_embedding, source_embedding)\n",
        "\n",
        "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
        "        reprogramming_embedding = torch.einsum(\"bhls,she->blhe\", A, value_embedding)\n",
        "\n",
        "        return reprogramming_embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Lhqi8qIB7UrH"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Config, GPT2Model, GPT2Tokenizer\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch\n",
        "\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class TimeLLM(nn.Module):\n",
        "    def __init__(self, configs=None):\n",
        "        super(TimeLLM, self).__init__()\n",
        "\n",
        "        self.configs = configs\n",
        "\n",
        "        self._set_llm_model()\n",
        "        self._set_tokenizer()\n",
        "        self._set_pad_token()\n",
        "        self._freeze_llm()\n",
        "        self.description = configs.content\n",
        "\n",
        "        # layers\n",
        "        self.dropout = nn.Dropout(configs.dropout)\n",
        "\n",
        "        self.patch_embedder = PatchEmbedder(\n",
        "            configs.d_model, configs.patch_len, configs.stride, configs.dropout\n",
        "        )\n",
        "\n",
        "        self.word_embeddings = self.llm_model.get_input_embeddings().weight\n",
        "        vocab_size = self.word_embeddings.shape[0]\n",
        "        num_tokens = 1000\n",
        "        self.mapping_layer = nn.Linear(vocab_size, num_tokens)\n",
        "\n",
        "        self.reprogramming_layer = ReprogrammingLayer(\n",
        "            configs.d_model, configs.n_heads, configs.d_ff, configs.llm_dim\n",
        "        )\n",
        "\n",
        "        self.patch_nums = int(\n",
        "            (configs.seq_len - configs.patch_len) / configs.stride + 2\n",
        "        )\n",
        "        self.head_nf = configs.d_ff * self.patch_nums\n",
        "\n",
        "        self.output_projection = FlattenHead(\n",
        "            self.head_nf, configs.pred_len, head_dropout=configs.dropout\n",
        "        )\n",
        "\n",
        "        self.normalize_layer = NormalizeLayer(configs.enc_in, affine=False)\n",
        "\n",
        "    def forward(self, x_enc) -> Tensor:\n",
        "        dec_out = self.forecast(x_enc)\n",
        "        return dec_out[:, -self.configs.pred_len :, :]\n",
        "\n",
        "    def forecast(self, x_enc) -> Tensor:\n",
        "        x_enc = self.normalize_layer(x_enc, mode=\"norm\")\n",
        "\n",
        "        B, T, N = x_enc.size()\n",
        "        x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n",
        "        prompt = self._generate_prompt(x_enc)\n",
        "        x_enc = x_enc.reshape(B, N, T).permute(0, 2, 1).contiguous()\n",
        "\n",
        "        prompt = self.tokenizer(\n",
        "            prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048\n",
        "        ).input_ids\n",
        "        prompt_embeddings = self.llm_model.get_input_embeddings()(\n",
        "            prompt.to(x_enc.device)\n",
        "        )  # (batch, prompt_token, dim)\n",
        "\n",
        "        source_embeddings = self.mapping_layer(\n",
        "            self.word_embeddings.permute(1, 0)\n",
        "        ).permute(1, 0)\n",
        "\n",
        "        x_enc = x_enc.permute(0, 2, 1).contiguous()\n",
        "        enc_out, n_vars = self.patch_embedder(x_enc.to(torch.bfloat16))\n",
        "        enc_out = self.reprogramming_layer(\n",
        "            enc_out, source_embeddings, source_embeddings\n",
        "        )\n",
        "        llm_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\n",
        "        dec_out = self.llm_model(inputs_embeds=llm_enc_out).last_hidden_state\n",
        "        dec_out = dec_out[:, :, : self.configs.d_ff]\n",
        "\n",
        "        dec_out = torch.reshape(\n",
        "            dec_out, (-1, n_vars, dec_out.shape[-2], dec_out.shape[-1])\n",
        "        )\n",
        "        dec_out = dec_out.permute(0, 1, 3, 2).contiguous()\n",
        "\n",
        "        dec_out = self.output_projection(dec_out[:, :, :, -self.patch_nums :])\n",
        "        dec_out = dec_out.permute(0, 2, 1).contiguous()\n",
        "\n",
        "        dec_out = self.normalize_layer(dec_out, \"denorm\")\n",
        "\n",
        "        return dec_out\n",
        "\n",
        "    def _calculate_lags(self, x_enc: Tensor, top_k: int = 5) -> Tensor:\n",
        "        q_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
        "        k_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
        "        res = q_fft * torch.conj(k_fft)\n",
        "        corr = torch.fft.irfft(res, dim=-1)\n",
        "        mean_value = torch.mean(corr, dim=1)\n",
        "        _, lags = torch.topk(mean_value, top_k, dim=-1)\n",
        "        return lags\n",
        "\n",
        "    def _generate_prompt(self, x_enc) -> List[str]:\n",
        "        min_values = torch.min(x_enc, dim=1)[0]\n",
        "        max_value = torch.max(x_enc, dim=1)[0]\n",
        "        medians = torch.max(x_enc, dim=1).values\n",
        "        lags = self._calculate_lags(x_enc)\n",
        "        trends = x_enc.diff(dim=1).sum(dim=1)\n",
        "\n",
        "        prompt = []\n",
        "        for batch in range(x_enc.shape[0]):\n",
        "            min_values_str = str(min_values[batch].tolist()[0])\n",
        "            max_values_str = str(max_value[batch].tolist()[0])\n",
        "            medians_str = str(medians[batch].tolist()[0])\n",
        "            lags_str = str(lags[batch].tolist())\n",
        "            prompt_ = (\n",
        "                f\"<|start_prompt|>Dataset description: {self.description}\"\n",
        "                f\"Task description: forecast the next {str(self.configs.pred_len)} steps given the previous {str(self.configs.seq_len)} steps information; \"\n",
        "                \"Input statistics: \"\n",
        "                f\"min value {min_values_str}, \"\n",
        "                f\"max value {max_values_str}, \"\n",
        "                f\"median value {medians_str}, \"\n",
        "                f\"the trend of input is {'upward' if trends[batch] > 0 else 'downward'}, \"\n",
        "                f\"top 5 lags are : {lags_str}<|<end_prompt>|>\"\n",
        "            )\n",
        "            prompt.append(prompt_)\n",
        "        return prompt\n",
        "\n",
        "    def _set_llm_model(self):\n",
        "       config = GPT2Config.from_pretrained(\n",
        "           \"gpt2\",\n",
        "           output_hidden_states=True,\n",
        "           cache_dir=\"./model/pretrained\"\n",
        "       )\n",
        "       try:\n",
        "           self.llm_model = GPT2Model.from_pretrained(\n",
        "               \"gpt2\",\n",
        "               config=config,\n",
        "               cache_dir=\"./model/pretrained\"\n",
        "           )\n",
        "       except Exception as e:\n",
        "         print(f\"Error loading GPT2 model: {e}\")\n",
        "\n",
        "    def _set_tokenizer(self):\n",
        "        try:\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "                \"openai-community/gpt2\",\n",
        "                trust_remote_code=True,\n",
        "                local_files_only=True,\n",
        "            )\n",
        "        except EnvironmentError:\n",
        "            print(\"Local tokenizer files not found. Attempting to download...\")\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "                \"openai-community/gpt2\",\n",
        "                trust_remote_code=True,\n",
        "                local_files_only=False,\n",
        "            )\n",
        "\n",
        "    def _set_pad_token(self):\n",
        "        if self.tokenizer.eos_token:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        else:\n",
        "            pad_token = \"[PAD]\"\n",
        "            self.tokenizer.add_special_tokens({\"pad_token\": pad_token})\n",
        "            self.tokenizer.pad_token = pad_token\n",
        "\n",
        "    def _freeze_llm(self):\n",
        "        for param in self.llm_model.parameters():\n",
        "            param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqB_nSbS7UrI"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Ups7yIJn7UrI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter  # noqa 5501\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple, Dict, List\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "def train(\n",
        "    args,\n",
        "    model: torch.nn.Module,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    valid_loader: torch.utils.data.DataLoader,\n",
        "    optim: torch.optim.Optimizer,\n",
        "    scheduler: torch.optim.lr_scheduler.OneCycleLR,\n",
        "    criterion: torch.nn.Module,\n",
        "    mae_metric: torch.nn.Module,\n",
        "    epochs: int = 10,\n",
        "    device: torch.device = \"cuda:0\",\n",
        "    writer: SummaryWriter = None,\n",
        ") -> Dict[str, List[float]]:\n",
        "    results = {\n",
        "        \"train_loss\": [],\n",
        "        \"valid_loss\": [],\n",
        "        \"mae_loss\": [],\n",
        "    }\n",
        "    for epoch in range(epochs):\n",
        "        start = timer()\n",
        "        train_loss = train_step(\n",
        "            args=args,\n",
        "            model=model,\n",
        "            dataloader=train_loader,\n",
        "            optim=optim,\n",
        "            scheduler=scheduler,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "        )\n",
        "        adjust_lr(optim, scheduler, epoch, args)\n",
        "\n",
        "        valid_loss, mae_loss = valid_step(\n",
        "            args=args,\n",
        "            model=model,\n",
        "            valid_loader=valid_loader,\n",
        "            criterion=criterion,\n",
        "            mae_metric=mae_metric,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"valid_loss\"].append(valid_loss)\n",
        "        results[\"mae_loss\"].append(mae_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\\nTime: {timer()-start}\")\n",
        "        print(f\"Train Loss: {train_loss}\\nValid Loss: {valid_loss}\")\n",
        "\n",
        "        if writer:\n",
        "            writer.add_scalars(\n",
        "                main_tag=\"Loss\",\n",
        "                tag_scalar_dict={\"train_loss\": train_loss, \"test_loss\": valid_loss},\n",
        "                global_step=epoch,\n",
        "            )\n",
        "\n",
        "            writer.add_scalars(\n",
        "                main_tag=\"MAE Loss\",\n",
        "                tag_scalar_dict={\"mae_loss\": mae_loss},\n",
        "                global_step=epoch,\n",
        "            )\n",
        "    if writer:\n",
        "        writer.close()\n",
        "    return results\n",
        "\n",
        "\n",
        "def train_step(\n",
        "    args,\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    optim: torch.optim.Optimizer,\n",
        "    scheduler: torch.optim.lr_scheduler.OneCycleLR,\n",
        "    criterion: torch.nn.Module,\n",
        "    device: torch.device = \"cuda:0\",\n",
        ") -> float:\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    print(\"Training...\")\n",
        "    for batch_idx, (batch_x, batch_y) in tqdm(enumerate(dataloader)):\n",
        "        # print(f\"Batch {batch_idx + 1}/{len(dataloader)} loaded.\")\n",
        "        batch_x = batch_x.float().to(device)\n",
        "        batch_y = batch_y[-1, :, :].float().to(device)\n",
        "\n",
        "        outputs = model(batch_x)[0]\n",
        "\n",
        "        f_dim = -1 if args.features == \"MS\" else 0\n",
        "        outputs = outputs[-args.pred_len :, f_dim:]\n",
        "        batch_y = batch_y[-args.pred_len :, f_dim:]\n",
        "\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "    return train_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def valid_step(\n",
        "    args,\n",
        "    model: torch.nn.Module,\n",
        "    valid_loader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    mae_metric: torch.nn.Module,\n",
        "    device: torch.device = \"cuda:0\",\n",
        ") -> Tuple[float, float]:\n",
        "    model.eval()\n",
        "    valid_loss, mae_loss = 0.0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        print(\"Testing...\")\n",
        "        for i, (batch_x, batch_y) in tqdm(enumerate(valid_loader)):\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y[-1, :, :].float().to(device)\n",
        "\n",
        "            outputs = model(batch_x)[0]\n",
        "\n",
        "            f_dim = -1 if args.features == \"MS\" else 0\n",
        "            outputs = outputs[-args.pred_len :, f_dim:]\n",
        "            batch_y = batch_y[-args.pred_len :, f_dim:]\n",
        "\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            valid_loss += loss.item()\n",
        "            mae_loss += mae_metric(outputs, batch_y).item()\n",
        "\n",
        "    return valid_loss / len(valid_loader), mae_loss / len(valid_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ6eR00-7UrJ"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "FwsH-L6H7UrJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def set_seeds(seed: int = 42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def parse_argument():\n",
        "    parser = argparse.ArgumentParser(description=\"TimeLLM\")\n",
        "    # data\n",
        "    parser.add_argument(\"--data\", type=str, default=\"ETT\", help=\"dataset type\")\n",
        "    parser.add_argument(\n",
        "        \"--embed\",\n",
        "        type=str,\n",
        "        default=\"timeF\",\n",
        "        help=\"time features encoding, options:[timeF, fixed, learned]\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--freq\", type=str, default=\"h\", help=\"freq for time features encoding\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--features\",\n",
        "        type=str,\n",
        "        default=\"M\",\n",
        "        help=\"forecasting task, options:[M, S, MS]; \"\n",
        "        \"M:multivariate predict multivariate, S: univariate predict univariate, \"\n",
        "        \"MS:multivariate predict univariate\",\n",
        "    )\n",
        "    parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
        "\n",
        "    # forecasting task\n",
        "    parser.add_argument(\"--seq_len\", type=int, default=96, help=\"input sequence length\")\n",
        "    parser.add_argument(\"--label_len\", type=int, default=48, help=\"start token length\")\n",
        "    parser.add_argument(\n",
        "        \"--pred_len\", type=int, default=96, help=\"prediction sequence length\"\n",
        "    )\n",
        "\n",
        "    # model define\n",
        "    parser.add_argument(\"--enc_in\", type=int, default=7, help=\"encoder input size\")\n",
        "    parser.add_argument(\"--d_model\", type=int, default=16, help=\"dimension of model\")\n",
        "    parser.add_argument(\"--n_heads\", type=int, default=8)\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--patch_len\", type=int, default=16)\n",
        "    parser.add_argument(\"--stride\", type=int, default=8)\n",
        "    parser.add_argument(\"--llm_layers\", type=int, default=6)\n",
        "    parser.add_argument(\"--d_ff\", type=int, default=32, help=\"dimension of fcn\")\n",
        "    parser.add_argument(\n",
        "        \"--llm_dim\", type=int, default=\"768\", help=\"LLM model dimension\"\n",
        "    )\n",
        "\n",
        "    # optimization\n",
        "    parser.add_argument(\"--train_epochs\", type=int, default=10)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "    parser.add_argument(\n",
        "        \"--patience\", type=int, default=10, help=\"early stopping patience\"\n",
        "    )\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=0.0001)\n",
        "    parser.add_argument(\"--pct_start\", type=float, default=0.2)\n",
        "    parser.add_argument(\"--percent\", type=int, default=100)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def adjust_lr(\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: torch.optim.lr_scheduler,\n",
        "    epoch: int,\n",
        "    args,\n",
        "):\n",
        "    new_lr = args.learning_rate * (0.5 ** ((epoch - 1) // 1))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = new_lr\n",
        "\n",
        "\n",
        "def load_content(args):\n",
        "    file = args.data\n",
        "    with open(\"/content/drive/My Drive/TimeLLM/{0}.txt\".format(file), \"r\") as f:\n",
        "        content = f.read()\n",
        "    return content\n",
        "\n",
        "\n",
        "def save_model(model: torch.nn.Module, model_name:str):\n",
        "    dir_path = Path(\"/content/drive/My Drive/TimeLLM/models\")\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    torch.save(model.state_dict(), dir_path / model_name)\n",
        "\n",
        "\n",
        "def test_data_loading(train_loader):\n",
        "    print(\"Testing data loading...\")\n",
        "    for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
        "        print(f\"Batch {batch_idx + 1}/{len(train_loader)} loaded.\")\n",
        "        if batch_idx >= 5:  # Testuj tylko kilka batchy\n",
        "            break\n",
        "    print(\"Data loading test completed.\")\n",
        "\n",
        "\n",
        "def create_writer(experiment_name: str,\n",
        "                  model_name: str,\n",
        "                  extra: str = None) -> SummaryWriter:\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    if extra:\n",
        "        # Create log directory path\n",
        "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra) # noqa 5501\n",
        "    else:\n",
        "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
        "\n",
        "    return SummaryWriter(log_dir=log_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaOGK8Xc7UrJ"
      },
      "source": [
        "# MAIN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rpGrZDWN7UrJ"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self, percent):\n",
        "        # Dataset parameters\n",
        "        self.data = \"ETT\"\n",
        "        self.embed = \"timeF\"\n",
        "        self.freq = \"h\"\n",
        "        self.features = \"M\"\n",
        "        self.target = \"OT\"\n",
        "\n",
        "        # Forecasting task parameters\n",
        "        self.seq_len = 96\n",
        "        self.label_len = 48\n",
        "        self.pred_len = 96\n",
        "        self.enc_in = 7\n",
        "        self.d_model = 16\n",
        "        self.n_heads = 8\n",
        "        self.dropout = 0.1\n",
        "        self.patch_len = 16\n",
        "        self.stride = 8\n",
        "        self.llm_layers = 6\n",
        "        self.d_ff = 32\n",
        "        self.llm_dim = 768\n",
        "\n",
        "        # Optimization parameters\n",
        "        self.train_epochs = 5\n",
        "        self.batch_size = 32\n",
        "        self.patience = 10\n",
        "        self.learning_rate = 0.0001\n",
        "        self.pct_start = 0.2\n",
        "        self.percent = percent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "EtgfX33C8trO",
        "outputId": "25848f11-009a-4815-d4db-fc1993bd2fca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "wSTNJFvW7UrK",
        "outputId": "3282400b-da85-4a1c-bdb1-1a9fa2aabe90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "1848it [31:46,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "609it [05:02,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Time: 2209.547333411\n",
            "Train Loss: 1.5105858388663274\n",
            "Valid Loss: 2.2296622449207946\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1848it [31:25,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "609it [04:58,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Time: 2183.7629621440005\n",
            "Train Loss: 1.557813240836064\n",
            "Valid Loss: 2.491021998321457\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1848it [31:23,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "609it [04:58,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Time: 2182.3394301709995\n",
            "Train Loss: 1.6397547021062715\n",
            "Valid Loss: 2.343665847255067\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1848it [31:24,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "609it [04:58,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Time: 2183.7708492640004\n",
            "Train Loss: 1.5874559445794494\n",
            "Valid Loss: 2.121670278226605\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1848it [31:26,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "609it [04:59,  2.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Time: 2185.9770305069997\n",
            "Train Loss: 1.594073527381711\n",
            "Valid Loss: 2.0348129779871167\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "def main():\n",
        "    set_seeds()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    args = Args(100)\n",
        "    args.content = load_content(args)\n",
        "    args.num_workers = os.cpu_count()\n",
        "    model = TimeLLM(args).float().to(device)\n",
        "\n",
        "    train_data, train_loader = data_provider(args, \"train\")\n",
        "    vali_data, vali_loader = data_provider(args, \"val\")\n",
        "    test_data, test_loader = data_provider(args, \"test\")\n",
        "    train_steps = len(train_loader)\n",
        "\n",
        "    trained_parameters = []\n",
        "    for p in model.parameters():\n",
        "        if p.requires_grad is True:\n",
        "            trained_parameters.append(p)\n",
        "\n",
        "    optim = torch.optim.Adam(trained_parameters, lr=args.learning_rate)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer=optim,\n",
        "        steps_per_epoch=train_steps,\n",
        "        pct_start=args.pct_start,\n",
        "        epochs=args.train_epochs,\n",
        "        max_lr=args.learning_rate,\n",
        "    )\n",
        "    criterion = nn.MSELoss()\n",
        "    mae_metric = nn.L1Loss()\n",
        "\n",
        "    results = train(\n",
        "        args=args,\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        optim=optim,\n",
        "        scheduler=scheduler,\n",
        "        criterion=criterion,\n",
        "        mae_metric=mae_metric,\n",
        "        device=device,\n",
        "        valid_loader=vali_loader,\n",
        "        epochs=args.train_epochs,\n",
        "        writer=create_writer(\"5_epochs\", \"time_llm_gpt2\")\n",
        "    )\n",
        "\n",
        "    save_model(model, \"time_llm_5_epochs.pth\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "!zip -r /content/model.zip /content/model\n",
        "files.download(\"/content/model.zip\")\n"
      ],
      "metadata": {
        "id": "N_IowEUfVebg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "59dc8ad6-acfe-440a-b566-8eb9de3cdb7d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/model/ (stored 0%)\n",
            "  adding: content/model/pretrained/ (stored 0%)\n",
            "  adding: content/model/pretrained/.locks/ (stored 0%)\n",
            "  adding: content/model/pretrained/.locks/models--gpt2/ (stored 0%)\n",
            "  adding: content/model/pretrained/.locks/models--gpt2/10c66461e4c109db5a2196bff4bb59be30396ed8.lock (stored 0%)\n",
            "  adding: content/model/pretrained/.locks/models--gpt2/248dfc3911869ec493c76e65bf2fcf7f615828b0254c12b473182f0f81d3a707.lock (stored 0%)\n",
            "  adding: content/model/pretrained/models--gpt2/ (stored 0%)\n",
            "  adding: content/model/pretrained/models--gpt2/blobs/ (stored 0%)\n",
            "  adding: content/model/pretrained/models--gpt2/blobs/10c66461e4c109db5a2196bff4bb59be30396ed8 (deflated 50%)\n",
            "  adding: content/model/pretrained/models--gpt2/blobs/248dfc3911869ec493c76e65bf2fcf7f615828b0254c12b473182f0f81d3a707 (deflated 16%)\n",
            "  adding: content/model/pretrained/models--gpt2/refs/ (stored 0%)\n",
            "  adding: content/model/pretrained/models--gpt2/refs/main (stored 0%)\n",
            "  adding: content/model/pretrained/models--gpt2/snapshots/ (stored 0%)\n",
            "  adding: content/model/pretrained/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/ (stored 0%)\n",
            "  adding: content/model/pretrained/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors (deflated 16%)\n",
            "  adding: content/model/pretrained/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json (deflated 50%)\n",
            "  adding: content/model/pretrained/time_llm_5_epochs.pth (deflated 7%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2f9cac48-d9b3-458f-93e3-6bf30236f009\", \"model.zip\", 1574865318)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate\n"
      ],
      "metadata": {
        "id": "hvPrN1ow4V1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "def plot_loss_curves(results: Dict[str, List[float]]) -> None:\n",
        "    \"\"\"\n",
        "    Plot training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(results[\"train_loss\"], label=\"Training loss\", color=\"blue\")\n",
        "    plt.plot(results[\"valid_loss\"], label=\"Validation loss\", color=\"red\")\n",
        "    plt.title(\"Loss curves\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(results[\"mae_loss\"], label=\"MAE loss\", color=\"green\")\n",
        "    plt.title(\"MAE loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "h6uImYRwVfEo"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "qthAMosp4d8A",
        "outputId": "98d75310-ce79-4615-c5b4-e2f390d049b9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'evaluate' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-634d456f050f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresoults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}